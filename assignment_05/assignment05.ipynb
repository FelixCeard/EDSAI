{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from utils import f, analytical_grad_wrt_w1, analytical_grad_wrt_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i assignment05_unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write down your names and matriculation numbers\n",
    "# ONLY Felix Ceard-Falkenberg (7007490)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Graph](function_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function graph above.The gradient of the function \n",
    "\\begin{align}\n",
    "f(x, y; w_1, w_2) = \\sin\\left( \\sqrt{\n",
    "\t\t\t\t\t\\left(w_1x \\right)^2 +\n",
    "\t\t\t\t\t\\left(w_2y \\right)^2 + \n",
    "\t\t\t\t\t\\left(w_1xw_2y \\right)^2\n",
    "\t\t\t\t\t\t\t} \\right)\n",
    "\\end{align}\n",
    "can be computed by making use of the function graph and locally computing gradients at each node. \n",
    "By doing this, we merely take advantage of the chain rule. Every node computes a forward pass and a backward pass.\n",
    "\n",
    "The forward pass is just the function applied on the input. The backward pass to the input node is the partial derivative of the current node with respect to its inputs times the backward input it receives from earlier on. So, for example, the last two nodes will compute the following values:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "sin &: \\begin{cases}\n",
    "\t\t\t\\text{Forward Inputs:}\\quad \\quad sqrt.forward\\_out \\\\\n",
    "\t\t\t\\text{Backward Inputs:}\\quad 1 \\\\\n",
    "\t\t\t\\text{Forward Output:}\\quad \\sin(sqrt.forward\\_out) \\\\\n",
    "\t\t\t\\text{Derivative w.r.t. $sqrt$:}\\quad \\cos(sqrt.forward\\_out)  \\\\\n",
    "\t\t\t\\text{Backward pass to $sqrt$:}\\quad \\cos(sqrt.forward\\_out) \\times 1  \\\\\n",
    "\t       \\end{cases}\\\\[2em]\n",
    "%\n",
    "sqrt &: \\begin{cases}\n",
    "\t\t\t\\text{Forward Inputs:}\\quad \\quad add_2.forward\\_out \\\\\n",
    "\t\t\t\\text{Backward Inputs:}\\quad  sin.backward\\_out\\\\\n",
    "\t\t\t\\text{Forward Output:}\\quad \\sqrt{add_2.forward\\_out} \\\\\n",
    "\t\t\t\\text{Derivative w.r.t. $add_2$:}\\quad 1/{(\\sqrt{add_2.forward\\_out})}  \\\\\n",
    "\t\t\t\\text{Backward pass to $add_2$:}\\quad 1/{(\\sqrt{add_2.forward\\_out})} \\times \\, \\text{backward input}  \\\\\n",
    "\t       \\end{cases}\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the \".\" refers to accessing the nodes computed values, so $sqrt.forward\\_out$ is just the output of the $sqrt$ node in the forward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following example for the addition node in the graph. Use the same structure to fill the gaps in the other nodes provided below. \n",
    "\n",
    "(Gaps are marked by TODO).\n",
    "\n",
    "Note that it is not important that you understand the concept of a class in Python. Just think of it as a box which holds certain values, for example the local gradients with respect to the input nodes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The __init__ function is called when you create a node, see below when we construct add1, for example.\n",
    "        # All functions of our node will always refer to the node they represent by 'self'.\n",
    "        # Hence, in the following, we just set the local gradients of the newly created node to None, as\n",
    "        # the node did not process any input yet.\n",
    "        \n",
    "        self.local_gradient_input_1 = None\n",
    "        self.local_gradient_input_2 = None\n",
    "\n",
    "    def forward(self, input_node1, input_node2):\n",
    "        # Compute the local gradients with respect to the input variables.\n",
    "        # The function add(a, b) = a + b has a derivative of 1 with respect to a and b\n",
    "        self.local_gradient_input_1 = 1\n",
    "        self.local_gradient_input_2 = 1\n",
    "        \n",
    "        # If you call the forward function of the node, (see later with add1.forward(a, b)),\n",
    "        # it will return a + b. This is the message that is sent to all subsequent nodes that \n",
    "        # get input from this node.\n",
    "        return input_node1 + input_node2\n",
    "    \n",
    "    def backward(self, backward_input):\n",
    "        # Make sure the local gradient is set. \n",
    "        # This means that we need to do a forward pass before a backward pass.\n",
    "        assert self.local_gradient_input_1 is not None and self.local_gradient_input_2 is not None \n",
    "        \n",
    "        # Now, we make use of the chain rule. Nodes later in the graph will provide the current value\n",
    "        # of the gradient, which has to be updated. \n",
    "        \n",
    "        # Hence, we multiply the local gradients with respect to the input nodes with the incoming backward signal\n",
    "        output_to_node_1 = backward_input * self.local_gradient_input_1\n",
    "        output_to_node_2 = backward_input * self.local_gradient_input_2\n",
    "        \n",
    "        # Each input to our node will get a backward signal, that depends on the gradient with \n",
    "        # respect to the input. In the case of AddNode, these are the same for both inputs.\n",
    "        # Of course, this is not generally the case for more complicated functions.\n",
    "        return output_to_node_1, output_to_node_2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.AddNodeTest) ... ok\n",
      "test_forward (__main__.AddNodeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of test_add...\n",
      "Equality holds.\n",
      "Testing equality of test_add.forward(12, 17) and 12+17...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b17f71aa30>"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'AddNodeTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the forward and backward function for the multiplication node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The __init__ function is called when you create a node, see below when we construct add1, for example.\n",
    "        # All functions of our node will always refer to the node they represent by 'self'.\n",
    "        # Hence, in the following, we just set the local gradients of the newly created node to None, as\n",
    "        # the node did not process any input yet.\n",
    "        self.local_gradient_input_1 = None\n",
    "        self.local_gradient_input_2 = None\n",
    "\n",
    "    def forward(self, input_node1, input_node2):\n",
    "        # Compute the local gradients with respect to the input variables.\n",
    "        # -> What is the derivative of the function with respect to its inputs?\n",
    "        self.local_gradient_input_1 = input_node2\n",
    "        self.local_gradient_input_2 = input_node1\n",
    "        \n",
    "        return input_node1 * input_node2\n",
    "    \n",
    "    def backward(self, backward_input):\n",
    "        # Make sure the local gradient is set. \n",
    "        # This means that we need to do a forward pass before a backward pass.\n",
    "        assert self.local_gradient_input_1 is not None and self.local_gradient_input_2 is not None \n",
    "        \n",
    "        # Multiply the local gradients with respect to the input nodes with the incoming backward signal\n",
    "        output_to_node_1 = backward_input * self.local_gradient_input_1\n",
    "        output_to_node_2 = backward_input * self.local_gradient_input_2\n",
    "        \n",
    "        return output_to_node_1, output_to_node_2\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.MulNodeTest) ... ok\n",
      "test_forward (__main__.MulNodeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.003s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of test_mul...\n",
      "Equality holds.\n",
      "Testing equality of test_mul.forward(14, 10) and 14*10...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b17f7054f0>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'MulNodeTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The __init__ function is called when you create a node, see below when we construct add1, for example.\n",
    "        # All functions of our node will always refer to the node they represent by 'self'.\n",
    "        # Hence, in the following, we just set the local gradients of the newly created node to None, as\n",
    "        # the node did not process any input yet.\n",
    "        self.local_gradient_input_1 = None\n",
    "        self.local_gradient_input_2 = None\n",
    "\n",
    "    def forward(self, input_node1):\n",
    "        # Compute the local gradients with respect to the input variables.\n",
    "        # -> What is the derivative of the function with respect to its inputs?\n",
    "        self.local_gradient_input_1 = 2 * input_node1\n",
    "        \n",
    "        return input_node1 ** 2\n",
    "    \n",
    "    def backward(self, backward_input):\n",
    "        # Make sure the local gradient is set. \n",
    "        # This means that we need to do a forward pass before a backward pass.\n",
    "        assert self.local_gradient_input_1 is not None\n",
    "        \n",
    "        # Multiply the local gradients with respect to the input nodes with the incoming backward signal\n",
    "        output_to_node_1 = backward_input * self.local_gradient_input_1\n",
    "\n",
    "        return output_to_node_1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.SquareNodeTest) ... ok\n",
      "test_forward (__main__.SquareNodeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of test_square...\n",
      "Equality holds.\n",
      "Testing equality of test_square.forward(18) and 18^2...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b17f71aeb0>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'SquareNodeTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquareRootNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The __init__ function is called when you create a node, see below when we construct add1, for example.\n",
    "        # All functions of our node will always refer to the node they represent by 'self'.\n",
    "        # Hence, in the following, we just set the local gradients of the newly created node to None, as\n",
    "        # the node did not process any input yet.\n",
    "        self.local_gradient_input_1 = None\n",
    "        self.local_gradient_input_2 = None\n",
    "\n",
    "    def forward(self, input_node1):\n",
    "        # Compute the local gradients with respect to the input variables.\n",
    "        # -> What is the derivative of the function with respect to its inputs?\n",
    "        # For efficiency, we can first compute the output and use it to set the gradient. \n",
    "        # Please write the gradient as a function of the output.\n",
    "        out = math.sqrt(input_node1)\n",
    "        self.local_gradient_input_1 = 1/(2*out)\n",
    "        return out\n",
    "    \n",
    "    def backward(self, backward_input):\n",
    "        # Make sure the local gradient is set. \n",
    "        # This means that we need to do a forward pass before a backward pass.\n",
    "        assert self.local_gradient_input_1 is not None\n",
    "        # Multiply the local gradients with respect to the input nodes with the incoming backward signal\n",
    "        output_to_node_1 = self.local_gradient_input_1  * backward_input\n",
    "        \n",
    "        return output_to_node_1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.SquareRootNodeTest) ... ok\n",
      "test_forward (__main__.SquareRootNodeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.004s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of test_squareroot...\n",
      "Equality holds.\n",
      "Testing equality of test_squareroot.forward([18]) and [18]^0.5 ...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b17f73fdf0>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'SquareRootNodeTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusNode:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # The __init__ function is called when you create a node, see below when we construct add1, for example.\n",
    "        # All functions of our node will always refer to the node they represent by 'self'.\n",
    "        # Hence, in the following, we just set the local gradients of the newly created node to None, as\n",
    "        # the node did not process any input yet.\n",
    "        self.local_gradient_input_1 = None\n",
    "        self.local_gradient_input_2 = None\n",
    "\n",
    "    def forward(self, input_node1):\n",
    "        # Compute the local gradients with respect to the input variables.\n",
    "        # -> What is the derivative of the function with respect to its inputs?\n",
    "        # Hint: For this function, (not for the previous ones!), \n",
    "        # you can use the functions provided in the math package. \n",
    "        # You can use the functions by calling math.sin, math.cos, math.sqrt etc...\n",
    "        self.local_gradient_input_1 = math.cos(input_node1)\n",
    "        \n",
    "        return math.sin(input_node1)\n",
    "    \n",
    "    def backward(self, backward_input):\n",
    "        # Make sure the local gradient is set. \n",
    "        # This means that we need to do a forward pass before a backward pass.\n",
    "        assert self.local_gradient_input_1 is not None\n",
    "        # Multiply the local gradients with respect to the input nodes with the incoming backward signal\n",
    "        output_to_node_1 = self.local_gradient_input_1 * backward_input\n",
    "        \n",
    "        return output_to_node_1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.SinusNodeTest) ... ok\n",
      "test_forward (__main__.SinusNodeTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.007s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing backward pass of test_sinus...\n",
      "Equality holds.\n",
      "Testing equality of test_sinus.forward([17]) and sinus([17])...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b17f7058b0>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'SinusNodeTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the nodes of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mul1 = MulNode()\n",
    "mul2 = MulNode()\n",
    "mul3 = MulNode()\n",
    "sq1 = SquareNode()\n",
    "sq2 = SquareNode()\n",
    "sq3 = SquareNode()\n",
    "add1 = AddNode()\n",
    "add2 = AddNode()\n",
    "sqrt = SquareRootNode()\n",
    "sin = SinusNode()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: You can verify your implementation by comparing the forward and backward outputs with your analytical solutions.\n",
    "Look at the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add1.forward(5, 6) == 5 + 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you first do a forward pass!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# If the gradient of the preceding node is 1, the gradient with respect to either input will be 1, too.\n",
    "# Note that the node returns 2 values, one for each node. We can split them up as follows\n",
    "gradient_wrt_node1, gradient_wrt_node2 = add1.backward(1)\n",
    "print(gradient_wrt_node1 == 1)\n",
    "print(gradient_wrt_node2 == 1)\n",
    "# More generally, as is easy to see, the addnode will just forward the gradients in the backward pass\n",
    "# to both input nodes.\n",
    "gradient_wrt_node1, gradient_wrt_node2 = add1.backward(5)\n",
    "print(gradient_wrt_node1 == 5)\n",
    "print(gradient_wrt_node2 == 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Creating the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_forward(x, y, w1, w2):\n",
    "    # Complete the forward pass according to the graph structure shown above.\n",
    "    mul1_out = mul1.forward(w1, x)\n",
    "    mul2_out = mul2.forward(w2, y)\n",
    "    mul3_out = mul3.forward(mul1_out, mul2_out)\n",
    "    sq1_out = sq1.forward(mul1_out)\n",
    "    sq2_out = sq2.forward(mul3_out)\n",
    "    sq3_out = sq3.forward(mul2_out)\n",
    "    add1_out = add1.forward(sq1_out, sq2_out)\n",
    "    add2_out = add2.forward(add1_out, sq3_out)\n",
    "    sqrt_out = sqrt.forward(add2_out)\n",
    "    sin_out = sin.forward(sqrt_out)\n",
    "    return sin_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation of the forward pass of the graph, we provided the analytical implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"><b>ATTENTION!</b></font> \n",
    "\n",
    "If you change the implementation of any of the nodes, you need to create the instantiations of the classes again (i.e., you need to run the cell with mul1 = ... again.) Otherwise, the graph will not include the update that you made to the Nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9003018938377099 1.03027441532284 1.4459543398331105 1.9840913514553762\n"
     ]
    }
   ],
   "source": [
    "# Creating 4 positive random numbers between 1 and 2. (np.random.random returns a number between 0 and 1)\n",
    "x, y, w1, w2 = np.random.random(4) + 1\n",
    "print(x, y, w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2911063565689682 0.2911063565689674\n"
     ]
    }
   ],
   "source": [
    "# f(x, y, w1, w2) is the direct computation of the result. Make sure that graph forward matches this!\n",
    "print(graph_forward(x, y, w1, w2), f(x, y, w1, w2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The following picture is the same as above. Just so you do not need to scroll if you want to see the graph.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Function Graph](function_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: The output of some nodes serves as input to more than 1 subsequent node. How do we combine the gradients coming from the different nodes? Look at this example:\n",
    "$$\n",
    "f(g(x)) = s(g(x)) + t(g(x))\\\\\n",
    "\\\\\n",
    "\\Rightarrow \\frac{\\partial f(g(x))}{\\partial g(x)} = \\quad?\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_backward():\n",
    "    # Complete the backward pass according to the graph structure shown above. \n",
    "    init_grad = 1\n",
    "    grad_wrt_sqrt = sin.backward(init_grad)\n",
    "    grad_wrt_add2 = sqrt.backward(grad_wrt_sqrt)\n",
    "    grad_wrt_add1, grad_wrt_sq3 = add2.backward(grad_wrt_add2)\n",
    "    grad_wrt_mul2_1 = sq3.backward(grad_wrt_sq3)\n",
    "    grad_wrt_sq1, grad_wrt_sq2 = add1.backward(grad_wrt_add1)\n",
    "    grad_wrt_mul3 = sq2.backward(grad_wrt_sq2)\n",
    "    grad_wrt_mul1_1, grad_wrt_mul2_2 = mul3.backward(grad_wrt_mul3)\n",
    "    grad_wrt_mul1_2 = sq1.backward(grad_wrt_sq1)\n",
    "\n",
    "    grad_wrt_mul1 = grad_wrt_mul1_1 + grad_wrt_mul1_2\n",
    "    grad_wrt_mul2 = grad_wrt_mul2_1 + grad_wrt_mul2_2\n",
    "    grad_wrt_w1, grad_wrt_x = mul1.backward(grad_wrt_mul1)\n",
    "    grad_wrt_w2, grad_wrt_y = mul2.backward(grad_wrt_mul2)\n",
    "    return grad_wrt_w1, grad_wrt_w2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_backward (__main__.GraphTest) ... ok\n",
      "test_forward (__main__.GraphTest) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.008s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing equality of graph_backward and analytical solution...\n",
      "Equality holds.\n",
      "Testing equality of graph_forward and analytical solution...\n",
      "Equality holds.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<unittest.main.TestProgram at 0x1b10b106850>"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unittest.main(argv=['ignored', '-v', 'GraphTest'], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your implementation of the backward pass, we provided the analytical implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_wrt_w1, grad_wrt_w2 = graph_backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to numerical instabilities, the values might not be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3.9323392772477015, 3.932339277247704)\n"
     ]
    }
   ],
   "source": [
    "print((grad_wrt_w1, analytical_grad_wrt_w1(x, y, w1, w2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(grad_wrt_w1, analytical_grad_wrt_w1(x, y, w1, w2)))\n",
    "print(np.allclose(grad_wrt_w2, analytical_grad_wrt_w2(x, y, w1, w2)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
